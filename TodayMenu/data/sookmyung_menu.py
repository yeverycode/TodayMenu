import requests
import csv
import json
import time

# ì¹´ì¹´ì˜¤ API ì„¤ì •
API_KEY = "815a330dcfb69987a6c219836b68598c"
headers = {"Authorization": f"KakaoAK {API_KEY}"}

# ë™ë³„ ì¤‘ì‹¬ ì¢Œí‘œ ì„¤ì •
areas = {
    "ê°ˆì›”ë™": (126.9722,37.54550),
    "ì²­íŒŒë™": (126.9668, 37.5450),
    "íš¨ì°½ë™": (126.9800, 37.5635),
    "ë‚¨ì˜ë™": (126.9737, 37.5427)
}

#  ê³µí†µ í‚¤ì›Œë“œ
queries = ["ìˆ ì§‘", "í˜¸í”„", "ì‹ë‹¹", "ë§›ì§‘", "ì¹´í˜", "ì£¼ì ", "í¬ì°¨", "ì–‘ì‹", "í•œì‹", "ì¤‘ì‹", "ì¼ì‹", "í“¨ì „", "ì™€ì¸ë°”", "í", "ë§¥ì£¼"]

# í‚¤ì›Œë“œ ê²€ìƒ‰ í•¨ìˆ˜
def search_keyword_places(query, x, y, radius=3000, page=1):
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    params = {
        "query": query,
        "x": x,
        "y": y,
        "radius": radius,
        "page": page,
        "size": 15
    }
    res = requests.get(url, headers=headers, params=params)
    print(f"[DEBUG] {query} - Page {page} - status: {res.status_code}")
    try:
        return res.json()
    except Exception as e:
        print("[ERROR] JSON ë””ì½”ë”© ì‹¤íŒ¨:", e)
        return {}

# í¬ë¡¤ë§ í•¨ìˆ˜ (ë™ í•„í„°ë§ í¬í•¨)
def crawl_keyword_places(queries, area_name, x, y, radius=3000, max_count=50):
    collected = []
    seen_ids = set()
    
    for query in queries:
        for page in range(1, 100):
            result = search_keyword_places(query, x, y, radius, page)
            documents = result.get("documents", [])
            print(f"[{area_name}] {query} - Page {page} - ìˆ˜ì‹ ëœ ê°€ê²Œ ìˆ˜: {len(documents)}")
            if not documents:
                break
            for place in documents:
                pid = place.get("id")
                address = place.get("address_name", "")

                # ì£¼ì†Œ í•„í„°ë§ (ë™ ì´ë¦„ í¬í•¨ ì—¬ë¶€)
                if area_name.replace("ë™", "") not in address:
                    continue

                if pid not in seen_ids:
                    seen_ids.add(pid)
                    collected.append(place)

                    # 50ê°œ ì±„ìš°ë©´ ì¢…ë£Œ
                    if len(collected) >= max_count:
                        return collected
            time.sleep(0.3)
    
    return collected

# CSV ì €ì¥
def save_to_csv(data, filename):
    with open(filename, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(["name", "address", "phone", "category", "lat", "lng", "id", "url"])
        for place in data:
            writer.writerow([
                place.get('place_name', ''),
                place.get('address_name', ''),
                place.get('phone', ''),
                place.get('category_name', ''),
                place.get('y', ''),
                place.get('x', ''),
                place.get('id', ''),
                place.get('place_url', '')
            ])

# JSON ì €ì¥
def save_to_json(data, filename):
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

# ì‹¤í–‰
if __name__ == "__main__":
    for area_name, (x, y) in areas.items():
        print(f"\nğŸ” [{area_name}] ì‹ë‹¹ ë° ìˆ ì§‘ ìˆ˜ì§‘ ì‹œì‘...")
        places = crawl_keyword_places(queries, area_name, x, y, radius=3000, max_count=50)
        print(f"[{area_name}] ìˆ˜ì§‘ ì™„ë£Œ: {len(places)}ê°œ")

        save_to_csv(places, f"{area_name}_finalì‹ë‹¹_50ê°œ.csv")
        save_to_json(places, f"{area_name}_finalì‹ë‹¹_50ê°œ.json")
        print(f"[{area_name}] ì €ì¥ ì™„ë£Œ (CSV & JSON)")
